## 内存优化技术

当然,业界还有一些其他的内存优化技术,比如Colossal-AI和LoRA。它们各有特点，相比之下,

**GaLore**，Gradient Low-Rank Projection，梯度低秩投影优势在于它既能支持全量更新,实现从头开始的完整预训练,又能显著降低内存占用,让普通消费级硬件也能训练大模型。这种全面而均衡的能力提升,是其他内存优化技术难以企及的。在论文中,研究人员使用70亿参数的LLaMA语言模型对GaLore进行了评估。结果显示,正常BF16精度训练LLaMA 7B模型,需要58GB显存。使用GaLore方法，结合8位量化优化器和逐层权重更新技术后,优化器状态的内存占用降低了65.5%,从42GB减少到了14.5GB。模型参数内存为14GB不变,但由于大幅降低了优化器状态和激活内存占用,最终总内存消耗仅为21.3GB。[原文](https://blog.csdn.net/ermu114/article/details/136621546)

**Colossal-AI**是一个支持分布式训练的框架。它通过数据并行、张量并行等策略,将大模型切分成多个部分,分散到不同的设备上训练。但这并没有减少训练的总内存需求,只是将压力分摊到了多个设备上。[原文](https://www.51cto.com/article/709231.html)

![来自网络截图](https://cdn.jsdelivr.net/gh/sylviara/sylviara.github.io@master/img/Glossal_AI_20240409130411.png)



**LoRA**则是在预训练模型的基础上,叠加一个小的低秩矩阵进行训练。这种方法减少了微调阶段需要更新的参数数量,但它的内存优势仅限于**微调阶段**,对**预训练阶段**无效。

## 模型量化剪枝-vLLM

[直接llama](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_quantization.ipynb)，通过quantization能将模型压缩，提升推理速度。

