tutorial：

1. 一个很好用的大语言模型可视化网站：

https://bbycroft.net/llm

![大语言模型可视化-GPT-2](https://cdn.jsdelivr.net/gh/sylviara/sylviara.github.io@master/img/20240429222358.png)

2. 讲解LLM量化/无GPU运行LLM比较出彩的台湾博客：

   https://blog.darkthread.net/blog/llama-cpp/

   ![关于量化的部分](https://cdn.jsdelivr.net/gh/sylviara/sylviara.github.io@master/img/20240430113101.png)

3. 紧跟着上一个博客，另一个将量化解释的比较全面的博客：

   https://ithelp.ithome.com.tw/m/articles/10330372

   ![讲解量化后的模型效果](https://cdn.jsdelivr.net/gh/sylviara/sylviara.github.io@master/img/20240430113443.png)

# 医学多模态基础模型

随着大语言模型的进展，[Moor 等人。](https://www.nature.com/articles/s41586-023-05881-4)提出了一种通用医学人工智能（GMAI），它可以解释多模态数据，例如成像、电子健康记录、实验室结果、基因组学、图表或医学文本。GMAI 以自我监督的方式对大型、多样化、多模态数据进行了预训练，可以进行多样化的医疗应用。

此外，[辛格尔等人。](https://www.nature.com/articles/s41586-023-06291-2)策划了医学领域的大规模问答数据集，并提出了基于[PaLM](https://arxiv.org/abs/2204.02311)（谷歌的大语言模型）的医学领域大语言模型，也称为Med-PaLM，这是第一个超过通过阈值的AI模型（ >60%）在美国医师执照考试（USMLE）中。几个月后，同一组作者提出了 Med-PaLM 的第二个版本（[Med-PaLM 2](https://arxiv.org/abs/2305.09617)）。如下图所示，Med-PaLM 2 实现了一个值得注意的里程碑（86.5% 对比 67.2% (Med-PaLM)），成为第一个在回答 USMLE 式问题方面达到与人类专家相当的熟练程度的水平。医生们注意到该模型对消费者医疗查询的长格式答案有了显着增强。

带标签的图像-文本对也为医学成像中的多模态学习提供了令人兴奋的机会。例如，Huang 等人通过对临床医生在医学 Twitter 等公共论坛上共享的图像文本对进行对比学习，开发了病理语言图像基础[模型](https://www.nature.com/articles/s41591-023-02504-3)。

## Multi-modal foundation models for medicine

Following the progress of the large language model,
[Moor et al.](https://www.nature.com/articles/s41586-023-05881-4) propose a generalist medical AI (GMAI), which can interpret multi-modal data such as imaging, electronic health records, laboratory results, genomics, graphs, or medical text. GMAI is pre-trained on large, diverse, multimodal data in a self-supervised manner and can conduct diverse medical applications.

Also, [Singhal et al.](https://www.nature.com/articles/s41586-023-06291-2) curates a large-scale question-answer dataset in the medical domain and proposes a medical domain large language model based on [PaLM](https://arxiv.org/abs/2204.02311) (Google’s large language models), also known as Med-PaLM, which is the first AI model that exceeds the pass threshold (>60%) in the U.S. Medical Licensing Examination (USMLE). After a couple of months, the same group of authors proposed the second version of Med-PaLM ([Med-PaLM 2](https://arxiv.org/abs/2305.09617)). As shown in the figure below, Med-PaLM 2 achieved a noteworthy milestone (86.5% versus 67.2% (Med-PaLM)) by becoming the first to attain a level of proficiency comparable to that of human experts in responding to USMLE-style questions. Physicians noted a significant enhancement in the model’s long-form answers to consumer medical queries.

Referrence来自[AI for Science](https://medium.com/@AI_for_Science/ai-for-science-in-2023-a-community-primer-d2c2db37e9a7#849a)在2023年的技术报告，学术引用格式如下：

> Stevens, R. et al. Ai for science. Tech. Rep., Argonne National Lab.(ANL), Argonne, IL (United States) (2020). --inspired from the 1st paper of "A review of some techniques for inclusion of domain-knowledge into deep neural networks"


## Medical LLM extended

The benchmark used in the study, MultilMed0A, comprises six open source datasets and an additional one on consumer medical questions, HealthsearchQA which we newly introduce. HealthsearchOA dataset is provided as a supplementary file.  [MedQA](https:/github.com/ind11/MedOA), [MedMCQA](https:/medmcga.github.io),[PubMedQA](https:/pubmedga.github.io), [LiveQA](https:/github.com/abachalLiveQA_MedicalTask_TREC2017), [MedicationQA](https://github.com/abachaa/Medication _QA_MedInfo2019), [MMLU](https://huggingface.co/datasets/hendrycks_test).



# What is 4-bit Quantization for Llama2 ?
[¶](https://www.kaggle.com/code/lorentzyeung/what-s-4-bit-quantization-how-does-it-help-llama2#What-is-4-bit-Quantization?)

Quantization in the context of deep learning is the process of constraining the number of bits that represent the weights and biases of the model. Weights and Biases numbers that we need in backpropagation.

In 4-bit quantization, each weight or bias is represented using only 4 bits as opposed to the typical 32 bits used in single-precision floating-point format (float32).

## Why does it use less GPU Memory?
The primary advantage of using 4-bit quantization is the reduction in model size and memory usage. Here's a simple explanation:

A float32 number takes up 32 bits of memory.
A 4-bit quantized number takes up only 4 bits of memory.
So, theoretically, you can fit 8 times more 4-bit quantized numbers into the same memory space as float32 numbers. This allows you to load larger models into the GPU memory or use smaller GPUs that might not have been able to handle the model otherwise.

The amount of memory used by an integer in a computer system is directly related to the number of bits used to represent that integer.

Memory Usage for 4-bit Integer:
A 4-bit integer uses 4 bits of memory.

Memory Usage for 32-bit Integer:
A 32-bit integer uses 32 bits of memory.

Conversion to Bytes
To convert these to bytes (since memory is often measured in bytes):

1 byte = 8 bits
A 4-bit integer would use ( 4/8 = 0.5 ) bytes.
A 16-bit integer would use ( 16/8 = 2 ) bytes.、


## 4 bit Will make the **accuracy lower** due to the quantification.
: 
`import numpy as np

`# Simulate original float32 weights
`original_weights = np.random.rand(1000).astype(np.float32)

`# Simulate 4-bit quantized weights
`# First, normalize the weights to a range of 0 to 15 (since 4 bits can represent 16 values)
`quantized_weights = np.round(original_weights * 15).astype(np.uint8)

`# De-normalize to get the approximated original weights
`approximated_weights = quantized_weights / 15.0

`# Calculate the error
`error = np.abs(original_weights - approximated_weights).mean()

`print(f"Average Quantization Error: {error}")

`original_weights

`quantized_weights

## Tools and tutorials for 4bit integration?
1. `pip install bitsandbytes`
In [this notebook](https://colab.research.google.com/drive/1Vvju5kOyBsDr7RX_YAvp6ZsSOoSMjhKD#scrollTo=XIyP_0r6zuVc), we will learn together how to load a model in 4bit, understand all its variants and how to run them for inference. 

[In the training notebook](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing), you will learn how to use 4bit models to fine-tune these models. 

If you liked the previous work for integrating [*LLM.int8*](https://arxiv.org/abs/2208.07339), you can have a look at the [introduction blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) to lean more about that quantization method.

Note that this could be used for any model**INCLUDING GPT** that supports `device_map` (i.e. loading the model with `accelerate`) - meaning that this totally agnostic to modalities, you can use it for `Blip2`, etc.

[from the llama2 original paper you could get:]()
## PPO: Proximal Policy Optimization

![image-20240428022424910](https://cdn.jsdelivr.net/gh/sylviara/sylviara.github.io@master/img/20240428022500.png)

[Reference](John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.)

## RLHF: Reinforcement Learning with Human Feedback

doulbe response from different model variants, let human decide which is [better]+[better degree] eg.`significantly better, better,`
`slightly better, or negligibly better/ unsure` (Touvron, 2023) according to criteria.